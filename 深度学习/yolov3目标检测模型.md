# yolov3目标检测模型

[TOC]



## 前言

### 基础概念

* **x_offset**：表示网格左上角相对x轴的距离（偏移量）
* **y_offset**：表示网格左上角相对y轴的距离（偏移量）
* **上采样**：缩小图像（或称为下采样（subsampled），如池化）的主要目的有两个：1、使得图像符合显示区域的大小；2、生成对应图像的缩略图。放大图像（或称为上采样（upSampling）或图像插值（interpolating））的主要目的是放大原图像,从而可以显示在更高分辨率的显示设备上。
* **先验框（anchor box）**：就是帮助我们定好了常见目标的宽和高，在进行预测的时候，我们可以利用这个已经定好的宽和高来进行处理，可以帮助我们进行预测，作用就是辅助处理x_offset、y_offset、h和w。如下图所示，用的是coco数据集，输出是(13,13,(80+5)*3)，乘3表示，有3个先验框，每个先验框都有85个参数，下图就有3个蓝色框，也即先验框，可以理解成给你的建议框，识别的对象可能在这些建议框中，目的是带你得到更高的IOU，即更高置信度、更可能有对象得部分，黄色框为真实最后显示的边界框，红色框表示中心位置。

![在这里插入图片描述](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/20201117204335594.png)

* **置信度（confidence）**：就是预测的先验框和真实框ground truth box（真实对象的框）的$IoU$值，即先验框是否有对象的概率Pr(Object)，如进行人脸识别，一张图中有房子，树，车，人等，识别时背景和人的身体都没有脸这个需要识别的对象，那么这些地方的置信度就是0，框中的人脸越多，置信度（有对象概率）就越大，置信度是检测中非常重要的参数。其中，$IoU$表示交并比，其公式如下：

$$
IoU = \frac {A \cap B}{A \cup B}
$$

![](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/2020111720441066.png)



## 一、预测部分

### 1、整体框架

![在这里插入图片描述](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/20201117210716527.png)

#### **主干特征提取网络**

YoloV3使用的主干特征提取网络是Darknet53，它具有两个重要特点：

1. Darknet53具有一个重要特点是使用了残差网络Residual，Darknet53中的残差卷积就是首先进行一次卷积核大小为3X3、步长为2的卷积，该卷积会压缩输入进来的特征层的宽和高，此时我们可以获得一个特征层，我们将该特征层命名为layer。之后我们再对该特征层进行一次1X1的卷积和一次3X3的卷积，并把这个结果加上layer，此时我们便构成了残差结构。通过不断的1X1卷积和3X3卷积以及残差边的叠加，我们便大幅度的加深了网络。残差网络的特点是容易优化，并且能够通过增加相当的深度来提高准确率。其内部的残差块使用了跳跃连接，缓解了在深度神经网络中增加深度带来的梯度消失问题。

2. Darknet53的每一个卷积部分使用了特有的DarknetConv2D结构，每一次卷积的时候进行L2正则化，完成卷积后进行BatchNormalization标准化与LeakyReLU。普通的ReLU是将所有的负值都设为零，LeakyReLU则是给所有负值赋予一个非零斜率。以数学的方式我们可以表示为：

![LeakyReLU](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/20191127153502635.png)

----------------------

### 2、从特征获取预测结果

* 构建**FPN特征金字塔进行加强特征提取**
* 利用**Yolo Head对三个有效特征层进行预测**

#### **利用FPN特征金字塔进行加强特征提取**：

在特征利用部分，YoloV3提取**多特征层进行目标检测**，一共**提取三个特征层**。三个特征层位于主干部分Darknet53的不同位置，分别位于**中间层，中下层，底层**，三个特征层的**shape分别为(52,52,256)、(26,26,512)、(13,13,1024)。**

<img src="https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/image-20211104220539634.png" alt="image-20211104220539634" style="zoom:50%;" />

在获得三个有效特征层后，我们利用这三个有效特征层进行FPN层的构建，构建方式为：

1. 13x13x1024的特征层进行5次卷积处理，处理完后**利用YoloHead获得预测结果**，**一部分用于进行上采样UmSampling2d后与26x26x512特征层进行结合**，结合特征层的shape为(26,26,768)。

2. 结合特征层再次进行5次卷积处理，处理完后**利用YoloHead获得预测结果**，**一部分用于进行上采样UmSampling2d后与52x52x256特征层进行结合**，结合特征层的shape为(52,52,384)。

3. 结合特征层再次进行5次卷积处理，处理完后**利用YoloHead获得预测结果**。

   ![image-20211104221224378](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/image-20211104221224378.png)

***特征金字塔可以将不同的shape的特征层进行特征融合，有利于提出更好地特征。***

#### 利用Yolo Head获得预测结果：

利用FPN特征金字塔，我们可以获得三个加强特征，**这三个加强特征的shape分别为(13,13,512)、(26,26,256)、(52,52,128)**，然后我们利用这三个shape的特征层传入Yolo Head获得预测结果。

**Yolo Head本质上是一次3x3卷积加上一次1x1卷积，3x3卷积的作用是特征整合，1x1卷积的作用是调整通道数。**

对三个特征层分别进行处理，假设我们预测是的VOC数据集，我们的输出层的shape分别为(13,13,75)，(26,26,75)，(52,52,75)，**最后一个维度为75是因为该图是基于voc数据集的，它的类为20种，YoloV3针对每一个特征层的每一个特征点存在3个先验框，所以预测结果的通道数为3x25；**其中，**25=20+1+4，20表示种类，1表示置信度(是否存在物体)，4表示调整的位置参数（x_offset, y_offset, height, width) 。**
如果使用的是coco训练集，类则为80种，最后的维度应该为255 = 3x85，三个特征层的shape为(13,13,255)，(26,26,255)，(52,52,255)。

其实际情况就是，输入N张416x416的图片，在经过多层的运算后，会输出三个shape分别为(N,13,13,255)，(N,26,26,255)，(N,52,52,255)的数据，对应每个图分为13x13、26x26、52x52的网格上3个先验框的位置。

--------------------------

### 3、预测结果的解码

由第二步我们可以获得三个特征层的预测结果，shape分别为：

* (N, 13, 13, 75) -> (N, 13, 13, 3, 25) -> (N, 13, 13, 3, 20+1+4)
* (N, 26, 26, 75) -> (N, 26, 26, 3, 25) -> (N, 26, 26, 3, 20+1+4)
* (N, 52, 52, 75) -> (N, 52, 52, 3, 25) -> (N, 52, 52, 3, 20+1+4)

每一个有效特征层**将整个图片分成与其长宽对应的网格**，如(N,13,13,75)的特征层就是将整个图像分成13x13个网格；然后从每个网格中心**建立多个先验框**，这些框是网络预先设定好的框，网络的预测结果会判断这些框内是否包含物体，以及这个物体的种类。

**其中3表示先验框的个数，20表示种类，1表示置信度(是否存在物体)，4表示调整的位置参数（x_offset, y_offset, height, width) 。分**

***这个预测结果并不对应物体最终的预测框在图片上的位置，还需对其进行解码***

#### 解码过程

网络学习到的（x_offset, y_offset, height, width) 并不代表预测框的最终位置，还需进行处理。

根据训练得到的预测值（x_offset, y_offset, height, width)即$t_x, t_y, t_w, t_h$计算最后得到的边界框的坐标$b_x, b_y$以及宽高$b_w, b_h$如图所示：

![](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/20201117204658450.png)

$(c_x,c_y)$：该点所在网格的左上角距离最左上角相差的格子数。
$(p_w,p_h)$：先验框的边长
$(t_x,t_y)$：目标中心点相对于该点所在网格左上角的偏移量
$(t_w,t_h)$：预测边框的宽和高
$σ$：激活函数，论文作者用的是sigmoid函数，[0,1]之间概率，之所以用sigmoid取代之前版本的softmax，原因是softmax会扩大最大类别概率值而抑制其他类别概率值 ，图解如下：

![在这里插入图片描述](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/20201117204813540.png)

### 4、得分排序与非极大抑制筛选

这一部分**基本上是所有目标检测通用的部分**。其对于每一个类进行判别，将最大概率的框筛选出来：

* 取出每一类得分大于self.obj_threshold（阈值）的框和得分。
* 利用框的位置和得分进行非极大抑制。最后可以得出概率最大的边界框，也就是最后显示出的框，如下几幅图，一步步筛选得到最终边界框

![在这里插入图片描述](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/20201117205200932.png)

找到第一个：

![在这里插入图片描述](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/20201117205345930.png)

找到第二个：

![在这里插入图片描述](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/20201117205420187.png)



--------------------------

### 5、在原图上进行绘制

通过第3步，我们可以得到预测框在原图上的位置，而且这些预测框都是经过筛选的，将筛选后的预测框绘制在图片上，就可以获得结果了。

---------------------

## 二、训练部分

### 1、计算loss所需参数

**在计算loss的时候，实际就是对网络预测结果pred和真实框情况target进行对比。**

----------------------

### 2、pred是什么

对于yolo3的模型来说，网络最后输出的内容就是三个特征层每个网格点对应的预测框及其种类，即三个特征层分别对应着图片被分为不同size的网格后，每个网格点上三个先验框对应的位置、置信度及其种类。即，**每个网格里都有三个先验框，每个先验框里有表示其位置、置信度和种类的数据。**
输出层的shape分别为(13,13,75)，(26,26,75)，(52,52,75)，最后一个维度为75是因为是基于voc数据集的，它的类为20种，yolo3只有针对每一个特征层存在3个先验框，所以最后维度为3x25；
如果使用的是coco训练集，类则为80种，最后的维度应该为255 = 3x85，三个特征层的shape为(13,13,255)，(26,26,255)，(52,52,255)。

---------------

### 3、target是什么

target就是一个真实图像中，真实框的情况。
第一个维度是batch_size，第二个维度是每一张图片里面真实框的数量，第三个维度内部是真实框的信息，包括位置以及种类。

---------------

### 4、loss的计算过程

对pred和target需要进行以下步骤：

1. 判断真实框在图片中的位置，判断其**属于哪一个网格点**去检测。判断真实框和这个特征点的**哪个先验框重合程度最高**。计算该网格点应该有怎么样的预测结果才能获得真实框，**与真实框重合度最高的先验框被用于作为正样本。**

2. **根据网络的预测结果获得预测框**，计算预测框和所有真实框的重合程度，如果重合程度大于一定门限，则将该预测框对应的先验框忽略。其余作为负样本。

3. 最终损失由三个部分组成：

   a、**正样本，编码后的长宽与x、y轴偏移量与预测值的差距**。

   b、**正样本，预测结果中置信度的值与1对比；负样本，预测结果中置信度的值与0对比**。

   c、**实际存在的框，种类预测结果与实际结果的对比**。

