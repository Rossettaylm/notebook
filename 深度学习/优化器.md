# 优化器

## 1、SGD（小批量随机梯度下降）

$$
g_t \leftarrow \bigtriangledown f_{B_t}(x_{t-1}) = \frac {1}{|B|}\sum_{i\in B_t}\nabla f_i(x_{t-1})
$$

来计算时间步$t$的小批量$B_t$上目标函数位于$x_{t-1}$处的梯度$g_t$​

![image-20211125234527693](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/image-20211125234527693.png)

```python
torch.optim.SGD(lr = 0.1)
```

## 2、动量法

>  指数加权移动平均

![image-20211125234515176](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/image-20211125234515176.png)

![image-20211205171447702](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/image-20211205171447702.png)

```python
torch.optim.SGD(lr = 0.1, momentum = 0.5)
```

动量法在每个时间步的自变量更新量近似于将最近$1/(1-\gamma)$个时间步的普通更新量（即学习率乘以梯度）做了指数加权移动平均后再除以$1-\gamma$。如$\gamma = 0.95$，则当前动量变化与近20个时间步的梯度变化有关，且离时间步$t$越近权重越大，关系越大。

![img](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/7.4_output1.png)

## 3、AdaGrad

> 使用一个小批量随机梯度按$g_t$​按元素平方的累加变量$s_t$。

**它根据自变量在每个维度的梯度值的大小来调整各个维度上的学习率，从而避免统一的学习率难以适应所有维度的问题**

需要强调的是，小批量随机梯度按元素平方的累加变量$s_t$出现在学习率的分母项中。因此，如果目标函数有关自变量中某个元素的偏导数一直都较大，那么该元素的学习率将下降较快；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么该元素的学习率将下降较慢。然而，由于$s_t$一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。所以，**当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到一个有用的解**。

![image-20211125234911020](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/image-20211125234911020.png)

![image-20211125234445154](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/image-20211125234445154.png)

```python
torch.optim.Adagrad(lr = 0.1)
```

![img](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/7.5_output1.png)

## 4、RMSProp

> 对$s_t$进行指数加权移动平均

因为RMSProp算法的状态变量$s_t$是对$g_t \odot g_t$的指数加权移动平均，所以可以看作是最近$1/(1-\gamma)$个时间步的小批量随机梯度平方项的加权平均。如此一来，自变量每个元素的学习率在迭代过程中就不再一直降低（或不变）。

![image-20211125234825091](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/image-20211125234825091.png)

![image-20211125235000313](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/image-20211125235000313.png)

```python
torch.optim.RMSProp(lr = 0.01, alpha = 0.9)
```

![img](https://rossetta-typora-imgsubmit.oss-cn-hangzhou.aliyuncs.com/img/7.6_output1.png)

## 5、AdaDelta

> 对AdaGrad后期难以找到可用解做了优化，**AdaDelta算法没有学习率**
>
> 使用了小批量随机梯度$g_t$按元素平方的指数加权移动平均变量$s_t$

在时间步$t=0$，它所有元素被初始化为0，给定超参数$0 \leq \rho < 1$（对应RMSProp算法中的$\gamma$），在时间步$t>0$，计算得
$$
s_t \leftarrow \rho s_{t-1} + (1-\rho)g_t \odot g_t
$$
与RMSProp算法不同的是，AdaDelta算法还额外维护一个状态变量$\Delta x_t$，其元素同样也在步0时被初始化为0.我们使用$\Delta x_{t-1}$来计算自变量的变化：
$$
g'_t \leftarrow \sqrt {\frac {\Delta x_{t-1} + \epsilon}{s_t + \epsilon}} \odot g_t
$$
其中$\epsilon$是为了维护数值稳定性而添加的常数，如$10^{-5}$。接着更新自变量：
$$
x_t \gets x_{t-1} - g'_t
$$
最后，使用$\Delta x_t$来记录自变量变化量$g'_t$按元素平方的指数加权移动平均：
$$
\Delta x_t \gets \rho \Delta x_{t-1}+(1-\rho)g'_t \odot g'_t
$$
可以看到，如不考虑$\epsilon$的影响，AdaDelta算法跟RMSProp算法的不同在于使用$\sqrt{\Delta x_{t-1}}$来代替学习率$\eta$

```python
torch.optim.Adadelta(rho=0.9)
```

## 6、Adam

Adam算法使用了**动量变量**$v_t$和RMSProp算法中**小批量随机梯度按元素平方的指数加权移动平均变量**$s_t$，并在时间步0将他们每个元素初始化为0。给定超参数$0 \leq \beta_1 < 1$（算法作者建议为0.9），时间步t的动量变量$v_t$即小批量随机梯度$g_t$的指数加权移动平均：
$$
v_t \gets \beta_1 v_{t-1} + (1-\beta_1)g_t
$$
和RMSProp算法中一样，给定超参数$0 \leq \beta_2 < 1$（算法作者建议为0.999），将小批量随机梯度按元素平方后的项$g_t \odot g_t$做指数加权移动平均得到$s_t$：
$$
s_t \gets \beta_2s_{t-1} + (1-\beta_2)g_t \odot g_t
$$
我们将$v_0$和$s_0$中的元素都初始化为0，<u>当$t$较小时，过去各时间步小批量随机梯度权值会较小。</u>
例如，当$\beta_1 = 0.9$时，$v1 = 0.1g_t$。为了消除这样的影响，对于任意时间步$t$，我们再将其除以$1-\beta_1^t$，从而使过去各时间步小批量随机梯度权值为之和为1，即$\sum_{t=0}^{n} g_t = 1$，这也叫**偏差修正**。
$$
\hat v_t \gets \frac {v_t}{1-\beta_1^t} \\
\hat s_t \gets \frac {s_t}{1-\beta_2^t}
$$
接下来，Adam算法通过上述的$\hat v_t \text{和} \hat s_t$，将模型参数中每个元素的学习率通过按元素运算重新调整：
$$
g'_t \gets \frac {\eta \hat v t}{\sqrt {\hat s t} + \epsilon}
$$
其中，$\eta$是学习率，$\epsilon$是为了维持数值稳定性的常数

和上述三种算法一样，目标函数中每个元素都拥有自己的学习率。最后使用$g'_t$迭代自变量：
$$
x_t \gets x_{t-1} - g'_t
$$

```python
torch.optim.Adam(lr = 0.01)
```

